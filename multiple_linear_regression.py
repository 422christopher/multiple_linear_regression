# -*- coding: utf-8 -*-
"""M2MulLinReg.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tJds7_SRNnO-RV0-kKu3_krGBXjCO4Ka

BloomTech Data Science

*Unit 2, Sprint 1, Module 2*

---
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
import sys
# 
# # If you're on Colab:
if 'google.colab' in sys.modules:
    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Applied-Modeling/master/data/'
# 
# If you're working locally:
else:
    DATA_PATH = '../data/'

"""# Module Project: Regression II

In this project, you'll continue working with the New York City rent dataset you used in the last module project.

## Directions

The tasks for this project are as follows:

- **Task 1:** Import `csv` file using `wrangle` function.
- **Task 2:** Conduct exploratory data analysis (EDA), and modify `wrangle` function to engineer two new features.
- **Task 3:** Split data into feature matrix `X` and target vector `y`.
- **Task 4:** Split feature matrix `X` and target vector `y` into training and test sets.
- **Task 5:** Establish the baseline mean absolute error for your dataset.
- **Task 6:** Build and train a `Linearregression` model.
- **Task 7:** Calculate the training and test mean absolute error for your model.
- **Task 8:** Calculate the training and test $R^2$ score for your model.
- **Stretch Goal:** Determine the three most important features for your linear regression model.

**Note**

You should limit yourself to the following libraries for this project:

- `matplotlib`
- `numpy`
- `pandas`
- `sklearn`
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error

"""# I. Wrangle Data"""

def wrangle(filepath):
    df = pd.read_csv(filepath)
    
    # Remove the most extreme 1% prices,
    # the most extreme .1% latitudes, &
    # the most extreme .1% longitudes
    df = df[(df['price'] >= np.percentile(df['price'], 0.5)) & 
            (df['price'] <= np.percentile(df['price'], 99.5)) & 
            (df['latitude'] >= np.percentile(df['latitude'], 0.05)) & 
            (df['latitude'] < np.percentile(df['latitude'], 99.95)) &
            (df['longitude'] >= np.percentile(df['longitude'], 0.05)) & 
            (df['longitude'] <= np.percentile(df['longitude'], 99.95))]

    # cols_to_drop = []
    # threshold = 100

    # for col in df:
    #   if df[col].dtype == 'object':
    #     if df[col].nunique() > threshold:
    #       cols_to_drop.append(df[col])

    # df.drop(columns = cols_to_drop, inplace = True)
    
    return df

filepath = DATA_PATH + 'apartments/renthop-nyc.csv'

#original shape
df = pd.read_csv(filepath)
df.shape

"""**Task 1:** Add the following functionality to the above `wrangle` function.

- The `'created'` column will parsed as a `DateTime` object and set as the `index` of the DataFrame. 
- Rows with `NaN` values will be dropped.

Then use your modified function to import the `renthop-nyc.csv` file into a DataFrame named `df`.
"""

df = wrangle(filepath)

df.shape

#convert the 'created' column from dtype 'object' to dtype 'datetime'
df['created'] = pd.to_datetime(df['created'])
df.dtypes

#make the 'created' column the index
df = df.set_index(df['created'])
df.head()

df.dropna(inplace=True)
df.shape

df.head()

"""**Task 2:** Using your `pandas` and dataviz skills decide on two features that you want to engineer for your dataset. Next, modify your `wrangle` function to add those features. 

**Note:** You can learn more about feature engineering [here](https://en.wikipedia.org/wiki/Feature_engineering). Here are some ideas for new features:

- Does the apartment have a description?
- Length of description.
- Total number of perks that apartment has.
- Are cats _or_ dogs allowed?
- Are cats _and_ dogs allowed?
- Total number of rooms (beds + baths).
"""

df['rooms'] = df['bathrooms'] + df['bedrooms']
df.shape

df['rooms']

plt.scatter(df['rooms'], df['price'])

import seaborn as sns

sns.lmplot(data=df, x='rooms', y='price')

"""# II. Split Data

**Task 3:** Split your DataFrame `df` into a feature matrix `X` and the target vector `y`. You want to predict `'price'`.

**Note:** In contrast to the last module project, this time you should include _all_ the numerical features in your dataset.
"""

df.select_dtypes('number').shape

target = 'price'
X = df.select_dtypes('number').drop(columns=target)
y = df[target]

X.shape

X.head()

y.head()
y.shape

"""**Task 4:** Split `X` and `y` into a training set (`X_train`, `y_train`) and a test set (`X_test`, `y_test`).

- Your training set should include data from April and May 2016. 
- Your test set should include data from June 2016.
"""

X.index

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
cutoff = '2016-06'
mark = X.index < cutoff

X_train = X.loc[mark]
y_train = y.loc[mark]

X_test = X.loc[~mark]
y_test = y.loc[~mark]

X_train, X_test, y_train, y_test

"""# III. Establish Baseline

**Task 5:** Since this is a **regression** problem, you need to calculate the baseline mean absolute error for your model. First, calculate the mean of `y_train`. Next, create a list `y_pred` that has the same length as `y_train` and where every item in the list is the mean. Finally, use `mean_absolute_error` to calculate your baseline.
"""

y_mean = y_train.mean()
y_mean_arr = [y_mean] * len(y_train)
baseline_mae = mean_absolute_error(y_train, y_mean_arr)
print('Baseline MAE:', baseline_mae)

"""# IV. Build Model

**Task 6:** Build and train a `LinearRegression` model named `model` using your feature matrix `X_train` and your target vector `y_train`.
"""

# Step 1: Import predictor class
from sklearn.linear_model import LinearRegression

# Step 2: Instantiate predictor
model = LinearRegression()

# Step 3: Fit predictor on the (training) data
model.fit(X_train, y_train)

"""# V. Check Metrics

**Task 7:** Calculate the training and test mean absolute error for your model.
"""

training_mae = mean_absolute_error(y_train, model.predict(X_train))
test_mae = mean_absolute_error(y_test, model.predict(X_test))

print('Training MAE:', training_mae)
print('Test MAE:', test_mae)

"""**Task 8:** Calculate the training and test $R^2$ score for your model."""

training_r2 = model.score(X_train, y_train)
test_r2 = model.score(X_test, y_test)

print('Training MAE:', training_r2)
print('Test MAE:', test_r2)

"""# VI. Communicate Results

**Stretch Goal:** What are the three most influential coefficients in your linear model? You should consider the _absolute value_ of each coefficient, so that it doesn't matter if it's positive or negative.
"""

coefficients = model.coef_
columns = X.columns

pd.Series(coefficients, index=columns).sort_values(key=abs).tail(3)

"""The number of bathrooms an apartment has is about 7 times more influential in the apartment's price than the number of bedrooms."""
# %%
